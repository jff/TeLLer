\documentclass[]{article}

\usepackage{amsmath}
\usepackage{amssymb}

\setlength{\parindent}{0pt}
\setlength{\parskip}{10pt plus 2pt minus 2pt}

\newcommand{\refp}[1]{(\ref{#1})}

\title{Technical Report: A Forward Chaining Linear Logic Theorem Prover}
\author{Andy Kitchen}

\date{June 2011}

\begin{document}

\maketitle

\section{Overview}

This project has three main goals. One, pedagogical in nature, to educate the
author on theorem proving and linear logic. Two, to create a tool for
interactive storytelling and narrative exploration following ideas
in~\cite{bosser2010linear}. Three, to create a basis to empirically analyse
the effects of computational perturbation on theorem proving in linear logic.

Narrative representation in linear logic is of interest because it can
naturally express narrative forms without the need to introduce extraneous
frame axioms~\cite{bosser2010linear}. Perturbation during theorem proving,
among other things, allows us quantify the robustness of a given theory. i.e.
Even with the absence or modification of formulas and proof steps; to what
extent will the conclusions still hold?

\section{Linear Logic}

Linear logic is a non-classical logic with interesting
properties~\cite{lincoln1992linear} It is essentially derived from classical
logic by limiting one's ability to arbitrarily increase or decrease the
multiplicity of formulae. e.g. in classical logic $A \wedge A$ is equivalent
to $A$ however in linear logic this is not the case. These rules are called
structural rules, linear logic is therefor called a substructural logic.

The effect of eliminating rules that govern the creation and destruction of
formula is that there is a `conservation' of formula as so they can be
interpreted intuitively as resources. This resource-like nature of formulae
means that theorem proving in linear logic has strong connections to planning
and model checking~\cite{LukeAAMAS2012}.

A good explanation of linear logic can found in \cite{girard1989proofs}
and discussion and extended sequent calculus in \cite{lincoln1992linear}

\subsection{Theorem Proving}

Generally one wants to answer the question: what conclusions follow from a
given premise? Or, in the setting of sequent calculus~\cite{girard1989proofs}:
what sequents can be formed by applying only valid judgements? There are two
well established ways to do this: forward chaining and backward chaining. Both
work recursively in small steps. Forward chaining starts with what is known
and expands this piece by piece. Each time extending or combining facts to
produce more novel facts. Backward chaining is goal directed. It starts with a
given goal and recursively splits this into sub-goals. Until a sub-goal is
reached that is simple enough to be easily supported or refuted by what is
known. In this implementation we chose forward chaining, because it is
exploratory in nature, one does not need to know beforehand the exact question
being asked. This ability to explore is well suited for interactive story
telling, because there is no exact query being issued by the user, instead a
series of actions is generated by the user which is then used as feedback to
direct the proof search. It also allows for the most straight forward
experimentation with computational perturbation, because the simple forward
inference rules can easily be manipulated.

In general terms, if a logic has a sequent calculus, forward chaining works by
using left-handed judgements and backward chaining using right-handed
judgements. For each left-handed judgement: the forward chaining process can
be learned by reading the judgment from bottom-to-top, e.g.

\begin{equation}\label{eqn:leftwith1}\tag{$\&\text{L}_1$}
    \frac{\Gamma, A \vdash \Sigma}{\Gamma, A \& B \vdash \Sigma}
\end{equation}
\begin{equation}\label{eqn:leftwith2}\tag{$\&\text{L}_2$}
    \frac{\Gamma, B \vdash \Sigma}{\Gamma, A \& B \vdash \Sigma}
\end{equation}

In \refp{eqn:leftwith1} if we currently know $A\&B$
then we can start searching for things that follow from $A$. We can also use
\refp{eqn:leftwith2} and start searching for things that follow from $B$. Thus
anything found on our search starting from $A$ or $B$ is entailed by $A\&B$.
This fits with our intuition of how a forward chaining rule for an and-like
connective should work.

\begin{equation}\label{eqn:leftPlus}\tag{$\oplus\text{L}$}
    \frac{\Gamma, A \vdash \Sigma   \qquad   \Gamma, B \vdash \Sigma}
         {\Gamma, A \oplus B \vdash \Sigma}
\end{equation}

In \refp{eqn:leftPlus} the meta-variable $\Sigma$ appears twice above the
line; thus only formulae found in a search starting from $A$ and a search
starting from $B$ are entailed by $A \oplus B$. This also fits with our
intuition of an or-like connective.

\begin{equation}\label{eqn:implication}\tag{$\multimap\text{L}$}
    \frac{\Gamma_1 \vdash A   \qquad   \Gamma_2, B \vdash \Sigma}
         {\Gamma_1, \Gamma_2, A \multimap B \vdash \Sigma}
\end{equation}

Another interesting case is (simplified) linear implication
\refp{eqn:implication}. Here the variable $A$ changes sides and appears in the
succedent of the sequent in the top-left. This judgement can be read roughly
as: To show that $\Sigma$ is entailed by an antecedent containing $A \multimap
B$ show it can entail $A$, then show $B$ entails $\Sigma$. This fits with our
intuition of how a modus ponens style deduction should work. One may also
notice the subscripts on $\Gamma$. In linear logic, formulae spent to prove
$A$ may not be reused to prove $\Sigma$ they are `used up', so to speak, like
a coins in a vending machine.

Dealing with this judgement leaves us with a problem. Once $A$ becomes part of
the succedent, we have a known query, which is not well suited for the
exploratory nature of forward-chaining. One obvious solution is to use
backward chaining in this case to decide if $A$ is entailed. In this
implementation we make a simple and pragmatic choice and limit the form of $A$
to a (multiplicative) conjunction of atoms giving \refp{eqn:implication*}.

\begin{equation}\label{eqn:implication*}\tag{$\multimap\text{L}^*$}
    \frac{\Gamma_1 \vdash a_1 \otimes a_2 \otimes ... \otimes a_n
            \qquad
          \Gamma_2, B \vdash \Sigma}
         {\Gamma_1, \Gamma_2, (a_1 \otimes a_2 \otimes ... \otimes a_n)
            \multimap B \vdash \Sigma}
\end{equation}

This choice greatly simplifies the theorem prover (a separate backward
chaining program does not need to be written) and does not limit the
expressive power of the logic much in practice.

In \refp{eqn:implication} one must split the formulae on the bottom
and redistribute them to the two sequents on top. This raises the
question of how they should be split. In this implementation we
choose the greedy approach and let one side consume all the formulae
that it requires then give what remains to the other side.

\subsection{Narrative Logic}

Currently the program allows one to explore a narrative via a process of
reification. When the theorem prover eliminates $A \& B$ (`internal choice' or
`additive conjunction'). The user is actually prompted to focus the search on
$A$ or $B$. For $A \oplus B$ (`external choice' or `additive disjunction') a
coin is tossed to decide what the outcome actually becomes. This allows the
user to interactively explore a linear logic theorem.

\section{Computation Architecture}

The theorem prover has been implemented the Haskell
programming~language~\cite{hudak1992report} compiled with the
GHC~\cite{marlow2004glasgow} and tested with
Quickcheck~\cite{claessen2000quickcheck}. Parsing formulas is done with the
Parsec~\cite{leijen2001parsec} parser combinator library.

The syntax of the logic is implemented with an recursive
abstract data type, it is augmented with a separate implication
connective to make it more expressive:

\begin{verbatim}
data Term 
     = Atom String
     | Not Term

     -- Multiplicatives
     | Term :*: Term  -- Conjunction
     | Term :$: Term  -- Disjunction
     | Term :-@: Term -- Linear Implication

     -- Additives
     | Term :&: Term  -- Conjunciton
     | Term :+: Term  -- Disjunction

     -- Exponentials
     | OfCourse Term
     | WhyNot   Term

     -- Units
     | Top
     | Bottom
     | One
     | Zero
\end{verbatim}

Formulas are simplified by a rewriting module where rewriting rules can be
declared and applied to a term (via unification). Here is an example of a set
of rules for De Morgan's laws:

\begin{verbatim}
dedual_rules = [
    "a^^"      --@ "a",
    "(a * b)^" --@ "a^ $ b^",
    "(a $ b)^" --@ "a^ * b^",
    "(a & b)^" --@ "a^ + b^",
    "(a + b)^" --@ "a^ & b^",

    "(a -@ b)^" --@ "a * b^",

    "(!a)^" --@ "?(a^)",
    "(?a)^" --@ "!(a^)",

    "1^" --@ "%",
    "0^" --@ "#",
    "#^" --@ "0",
    "%^" --@ "1"
  ]
\end{verbatim}

Finally we need to apply the rules we have derived from the judgments in the
sequent calculus. Because on the bottom of a judgement we have one term
that is singled out, or `pointed', we can apply a function to all such
pointings with a pointed map:

\begin{verbatim}
    pointedMap f ls = map f (point ls)
    
    point ls = go [] ls
      where go prev (x:next) = (x, prev ++ next) : go (x:prev) next
            go prev [] = []
\end{verbatim}

A rule matches a pointed list and may rewrite it or not:

\begin{verbatim}
    type Rule = (Term, [Term]) -> Maybe [Term]
\end{verbatim}

We need to try a rule with each term pointed. A rule may match somewhere or
not at all:

\begin{verbatim}
    tryReduction :: Rule -> [Term] -> Maybe [Term]
    tryReduction f ls = msum (pointedMap f ls)  
\end{verbatim}

A rule function is then created for each sequent, and can be applied in this
framework. In the actual program,because rules may interact with the user,
each function above must be lifted into the IO monad, however their
characteristic types remain the same.

\section{Further work}

Right now users can interact with a narrative formula, but it would be
desirable to add a more humanistic interface where each atom could have a
gloss associated with it and formulae and reification points could be
explained in natural language.

The theorem prover could have numerous improvements, however most
important/interesting would be:

\begin{itemize}
    \item Add efficient support for exponential connectives
    \item Improve rules for units
    \item Add backtracking over formula splitting decisions
    \item Support arbitrary left-hand-sides for linear implication
\end{itemize}

Ultimately this software will serve as a base for experimentation with
computational perturbation. The author hopes most of his future
work will be in this area.

\bibliography{bibliography}{}
\bibliographystyle{plain}

\end{document}